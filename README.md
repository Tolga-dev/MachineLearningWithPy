# This repo is on work!
### If you want to contribute, open a branch for yourself and push!

### General ML learning

to do list
    -   https://machinelearningmastery.com/start-here/#calculus su siteyi bitir
    -   kitap var ne yapsak bilmiyorum onu - buyuk ihtimalle onu da bitirmem gerekecek ez
    -   https://github.com/AssemblyAI-Examples/Machine-Learning-From-Scratch/blob/main/08%20Perceptron/perceptron.py



## Road Map
- read Hands on machine learning with scikit learn  keras and tensorflow book


### Supervised
* you feed to the algorithm includes the desired solutions, called labels.
* A typical supervised learning task is classification
* regression, to train the system you need to give it many examples of data, predictors and their labels.
#### Examplesgit
* k-Nearest NeighborsLinear Regression
* Logistic Regression
* Support Vector Machines (SVMs)
* Decision Trees and Random Forests
* Neural networks

### UnSupervised learning
* System tries to learn without a teacher
#### Examples
* Clustering
  * K-Means
  * DBSCAN
  * Hierarchical Cluster Analysis (HCA)
* Anomaly detection and novelty detection
  * One-class SVM
  * Isolation Forest
* Visualization and dimensionality reduction
  * Principal Component Analysis (PCA)
    * Kernel PCA
  * Locally Linear Embedding (LLE)
  * t-Distributed Stochastic Neighbor Embedding (t-SNE)
* Association rule learning
  * Apriori
  * Eclat

* Use dimensional reduction algorithm
  * before you feed model to another machine learning algorithm
* Batch learning, 
  * incapable of learning incrementally, it must be trained using all the available data
* Online learning
  * mini batches, if u have limited coputing resources, once it has learned about new data instances, it does not need them anymore


* two main approaches to generalization:
  * instance based - learning
  * Model based - learning



### How to dive into ML model

* Examination of data
* Making a test data from main data
* 



  
### math 
* study vectors, 
* matrics, 
* Symmetric Matrices, 
* Orthogonalization & Orthonormalization, 
* Matrix Operations, 
* Projections, 
* Eigenvalues & Eigenvectors, 
* Vector Spaces, 
* Norms 

### Algos

* Principal Component Analysis (PCA), 
* Eigendecomposition of a matrix, 
* LU Decomposition, 
* QR Decomposition/Factorization, 
* Singular Value Decomposition (SVD)

### libs in python
* Numpy
* Pandas
* Matplotlib
* Seaborn
* TensorFlow
* Keras
* Scikit-learn
* PyTorch
* CV2

### concepts

Along with these seven steps, you need to master of following concepts and algorithms.
* Clean Data
* Fill Missing Value
* Drop Some Feature
* Feature Selection
* Feature Scaling
* Regularization
* Feature Engineering (optional at first)
* Regression Algorithms
* Simple Linear Regression
* Ridge & Lasso
* Multiple Linear Regression
* Polynomial Regression
* XGBRegressor
* Classification Algorithms
* KNN (K Nearest Neighbor)
* Logistic Regression
* Decision Tree
* Random Forest
* Naive Bayes
* XGBClassifier
* Clustering Algorithms
* K-Means
* DBSCAN (Density-based spatial clustering of applications)
* Dimensionality Reduction
* PCA (Principale Component Analysis)
* LDA
* t-SNE


* 4:34       -  Fundamentals of ML
* 25:22     -  Supervised VS Unsupervised
* 35:39     -  Linear Regression
* 1:07:06  -  Logistic Regression
* 1:24:12  -  Project: House price predictor
* 1:45:16  -  Regularization
* 2:01:12  -  Support vector machines
* 2:29:55  -  Project: Stock price predictor
* 3:05:55  -  Principal component analysis
* 3:29:14  -  Learning theory
* 3:47:38  -  Decision trees
* 4:58:19  -  Ensemble learning
* 5:53:28  -  Boosting, pt 1
* 6:11:16  -  Boosting, pt 2
* 6:44:10  -  Stacking Ensemble Learning
* 7:09:52  -  Unsupervised Learning, pt 1
* 7:26:58  -  Unsupervised Learning, pt 2
* 7:55:16  -  K-Means
* 8:20:21  -  Hierarchical Clustering
* 8:50:28  -  Project: Heart failure prediction
* 9:33:29  -  Project: Spam/Ham Detector


* ⌨️ (0:00:00) Intro
* ⌨️ (0:00:58) Data/Colab Intro
* ⌨️ (0:08:45) Intro to Machine Learning
* ⌨️ (0:12:26) Features
* ⌨️ (0:17:23) Classification/Regression
* ⌨️ (0:19:57) Training Model
* ⌨️ (0:30:57) Preparing Data
* ⌨️ (0:44:43) K-Nearest Neighbors
* ⌨️ (0:52:42) KNN Implementation
* ⌨️ (1:08:43) Naive Bayes
* ⌨️ (1:17:30) Naive Bayes Implementation
* ⌨️ (1:19:22) Logistic Regression
* ⌨️ (1:27:56) Log Regression Implementation
* ⌨️ (1:29:13) Support Vector Machine
* ⌨️ (1:37:54) SVM Implementation
* ⌨️ (1:39:44) Neural Networks
* ⌨️ (1:47:57) Tensorflow
* ⌨️ (1:49:50) Classification NN using Tensorflow
* ⌨️ (2:10:12) Linear Regression
* ⌨️ (2:34:54) Lin Regression Implementation
* ⌨️ (2:57:44) Lin Regression using a Neuron
* ⌨️ (3:00:15) Regression NN using Tensorflow
* ⌨️ (3:13:13) K-Means Clustering
* ⌨️ (3:23:46) Principal Component Analysis
* ⌨️ (3:33:54) K-Means and PCA Implementations

my machine learning roadmap



# Categories of learning

supervides - unsupervised learning

transfer learning

reinforcememnt learning

# Problem domains 

classification - regression - clustering - dimensionalty reduction


# tools 

a lot 


# math 

linear algebra matrix manipulation multivariate calculus chain rule probability and distributions, optimizations

jupyter - colab
numpy - pandas - scikit learn
hands on ml with scikit learn keras tensorflow
milestone project 1

fast.ai deep learning for coders 
tensorflow in pranctice 
full stack


./ missing semester cs degree

google cloud 

free code camp 

arXiv org


 # useful
sotabench - papers with code - made with ml
https://jasonbenn.com/


### General AI set 
* AI((Machine learning(Deep learning, ...)), ...)


1. Algorithm Descriptions
Here is an overview of the linear, nonlinear and ensemble algorithm descriptions:

* Algorithm 1: Gradient Descent.
* Algorithm 2: Linear Regression.
* Algorithm 3: Logistic Regression.
* Algorithm 4: Linear Discriminant Analysis.
* Algorithm 5: Classification and Regression Trees.
* Algorithm 6: Naive Bayes.
* Algorithm 7: K-Nearest Neighbors.
* Algorithm 8: Learning Vector Quantization.
* Algorithm 9: Support Vector Machines.
* Algorithm 10: Bagged Decision Trees and Random Forest.
* Algorithm 11: Boosting and AdaBoost.

1. Algorithm Tutorials

* Tutorial 1: Simple Linear Regression using Statistics.
* Tutorial 2: Simple Linear Regression with Gradient Descent.
* Tutorial 3: Logistic Regression with Gradient Descent.
* Tutorial 4: Linear Discriminant Analysis using Statistics.
* Tutorial 5: Classification and Regression Trees with Gini.
* Tutorial 6: Naive Bayes for Categorical Data.
* Tutorial 7: Gaussian Naive Bayes for Real-Valued Data.
* Tutorial 8: K-Nearest Neighbors for Classification.
* Tutorial 9: Learning Vector Quantization for Classification.
* Tutorial 10: Support Vector Machines with Gradient Descent.
* Tutorial 11: Bagged Classification and Regression Trees.
* Tutorial 12: AdaBoost for Classification.


* Linear regression for regression problems.
* Random forest for classification and regression problems.
* Support vector machines for classification problems.


* k-means for clustering problems.
* Apriori algorithm for association rule learning problems.
* LDA for topic modeling of text passages, i.e., discover and associate keywords to text.


## Algorithm 1: Gradient Descent

it is a simple optimization technique. 
If a model is predicting apartment prices


### What is the data normalization and standardization

### Normalization
Scale a variable to have values between 0 and 1
### Standardization 
transforms data to have a mean of zero and a standard deviation of 1. This is called z-score and data points can be standardized with the following formula:

```
    x = []
    z = (x - x`) / std
```
They allow you to compare different sets of data and to find probabilities for sets of data using standardized tables (called z-tables).



